/*************************************
 *
 * 撰寫人: 孫培元
 * 撰寫日期: 2019-08-22
 * 撰寫人電郵: 404657164@gms.tku.edu.tw
 *
 *************************************/

------------------------------------------------------------------------------------------------------

[SinicaCrawler/ 資料夾結構示意圖]
.
├── LICENSE                                                 # 授權條款
├── README.md                                               # 專案說明markdown
├── crawler                                                 # 放爬蟲專案的資料夾
│   ├── serv_gcis_nat_gov_tw_moeadsBF
│   ├── serv_gcis_nat_gov_tw_pub
│   └── www_iyp_com_tw
├── polipo-20140107-win32.zip                               # 相依程式: polipo for windows
├── polipo.conf                                             # 相依程式設定檔: polipo
├── requestment.txt                                         # python會用到的套件
├── startup.sh                                              # debian/ubuntu系統部署腳本
├── system                                                  # debian/ubuntu系統部署腳本
│   ├── installpython.sh
│   └── startup.sh
├── tor-win32-0.4.0.5.zip                                   # 相依程式:tor for windows
├── torrc                                                   # 相依程式設定檔: tor
├── venv                                                    # python虛擬環境
│   ├── bin
│   ├── include
│   └── lib
└── 交接文件                                                 # 交接以及其他說明文件
    └── 資料夾結構.txt

------------------------------------------------------------------------------------------------------
[說明]

(一) requestment.txt 與 venv/

requestment.txt 是 python 套件管理的設定檔，在安裝套件之前，請先在 python3.7.4 上安裝 virtualenv
套件，然後在 SinicaCrawler/ 資料夾底下(終端機環境中)使用:

    > virtualenv venv

指令創建 python 虛擬環境，這時 SinicaCrawler/ 資料夾底下會多出 venv/ 資料夾，但我們要如何使用虛擬環境中的 python？

Windows:
    > source venv/Scripts/activate

macOS/linux:
    > source venv/bin/activate

使用以上指令後，你的終端機開頭會多出(venv)$ 的字樣，之後再透過:

    > pip install -r requestment.txt

來安裝python相依套件，這樣子我們的python環境已經建立完成了。


[相關說明]

virtualenv官方網站說明 => https://virtualenv.pypa.io/en/latest/


(二) crawler/ 資料夾

為了讓程式能夠長期維護以及管理，所以訂定一些規範來管理程式碼以及文件，crawler資料夾
是放置爬蟲的資料夾，爬蟲專案我們盡量使用scrapy來開發（scrapy是一個python crawler
framework），理由是因為框架具有一定架構，我們只要按照框架原始規範去開發，就能完成我們
要做的事情，以下是軟體框架的優缺點:

優點:
    1. 不必重造輪子，別人已經幫我們完成的我們可以直接套用
    2. 具有結構性，日後比較好維護
    3. 框架大多含有測試機制，便於除蟲

缺點:
    1. 被框架規範侷限住，無法完全按照自己的方法開發
    2. 如果遇到問題，需要檢查是否為框架本身的問題
    3. 不當使用會導致大量的消耗系統資源

大致說明完框架的利弊後，接下來要講解如何建立專案，假設我們終端機位置在SinicaCrawler/crawler
底下(可使用指令確認位置，unix: pwd | windows: dir)，我們要建立新的爬蟲專案，專案名稱盡量以
要爬取網站之連結做命名，譬如: https://www.example.com 我們就命名為 www_example_com。
我們可以利用scrapy的cli功能建立專案:

    > scrapy startproject www_example_com

這時crawler資料夾底下會出現www_example_com資料夾:

[SinicaCrawler/crawler 資料夾結構示意圖]
.
└── www_example_com                         # 專案資料夾
    ├── scrapy.cfg                          # 部署配置文件
    └── www_example_com                     # 爬蟲資料夾
        ├── __init__.py
        ├── __pycache__                     # python快取檔存放位置
        ├── items.py                        # project items definition file
        ├── middlewares.py                  # project middlewares file
        ├── pipelines.py                    # project pipelines file
        ├── settings.py                     # 爬蟲設定檔
        └── spiders                         # 爬蟲腳本資料夾
            ├── __init__.py
            └── __pycache__

專案創建成功後，將終端機位置移動至SinicaCrawler/crawler/www_example_com/www_example_com底下，
由於未來爬蟲會抓取資料以及資料清洗，所以我們要再手動在SinicaCrawler/crawler/www_example_com/www_example_com
底下創建data/和datacleaning/資料夾，以便未來存放資料和清洗資料，使用以下指令建立爬蟲腳本和建立資料夾:

    > scrapy genspider crawler www.example.com
    > mkdir data/ datacleaning/

這時專案會變成:

[SinicaCrawler/crawler 資料夾結構示意圖]
.
└── www_example_com
    ├── scrapy.cfg
    └── www_example_com
        ├── __init__.py
        ├── __pycache__
        │   ├── __init__.cpython-37.pyc
        │   └── settings.cpython-37.pyc
        ├── data                           # 爬蟲放取資料的地方
        ├── datacleaning                   # 資料清洗的資料夾，資料清洗以R語言為主(建議使用 Rstudio 1.2.1335以上之版本)
        ├── items.py
        ├── middlewares.py
        ├── pipelines.py
        ├── settings.py
        └── spiders
            ├── __init__.py
            ├── __pycache__
            └── crawler.py                 # 爬蟲主要腳本，此腳本會使用到上層目錄之檔案

之後我們要來建立資料清理專案，請先安裝 R 語言(3.6.1)以及 Rstudio，然後再安裝 packrat 套件:

    > install.packages('packrat')

R 的版本控管機制比 python麻煩一點，所以我們分兩個部分探討:

    1. 初始建立datacleaning
    2. git clone 下來後部署環境

(1) 初始建立datacleaning

安裝好相關軟體後，開啟 Rstudio，並點選 Rstudio 視窗上方選項，File -> New Project -> Existing Directory ->
[選擇SinicaCrawler/crawler/www_example_com/www_example_com/datacleaning] -> 點選 Create Project 按鈕，之後在 R cmd 上輸入:

    > packrat::init()

來建立R虛擬環境，此時 R 的 Library 都會安裝在 datacleaning/packrat 之下，假設要安裝ggplot:

    > install.packages('ggplot2')

可以利用以下程式碼來記錄安裝的套件版本:

    > packrat::snapshot()
    // Adding these packages to packrat:
    //              _
    // R6             2.4.0
    // RColorBrewer   1.1-2
    // Rcpp           1.0.2
    // assertthat     0.2.1
    // backports      1.1.4
    // cli            1.1.0
    // colorspace     1.4-1
    // crayon         1.3.4
    // digest         0.6.20
    // ellipsis       0.2.0.1
    // fansi          0.4.0
    // ggplot2        3.2.1
    // glue           1.3.1
    // ............
    // ......

這時 datacleaning/packrat/packrat.lock 會紀錄剛剛安裝的套件版本資訊，snapshot 會紀錄剛剛所安裝的套件，對於長期維運非常的有幫助，
同時在 datacleaning/packrat 下會有個 .gitignore，我們在裡面加入:

    packrat/lib*/
    packrat/init.R
    packrat/packrat.opts
    packrat/src
    .Rhistory

.gitignore 是一個隱藏檔，它能協助我們忽略上傳指定的檔案，譬如說有1.xxGb的資料，我們不希望把這麼龐大的資料公開出去，則可以在資料夾底下新增
此檔案，並寫下我們要忽略的檔案，未來在提交程式碼時， git 會自動幫我們忽略不必要上傳的檔案，所以未來提交專案時， datacleaning/packrat
只會有 packrat.lock(紀錄套件的檔案)。

(2) git clone 下來後部署環境

如果今天從git server clone下來，我們要怎麼重新部署環境？我們先用 Rstudio 開啟原先 datacleaning 專案，點選 Rstudio 視窗上方選項，
File -> Open Project -> [選擇SinicaCrawler/crawler/www_example_com/www_example_com/datacleaning/datacleaning.Rproj]，
專案就會自動啟動，之後透過 init 重新建立虛擬環境( R 和 python 建立虛擬環境的原理就是把整個原生直譯器複製到指定資料夾底下):

    > packrat::init()

由於過去我們已經建立 packrat.lock，所以接下來只要把指定套件下載下來就好:

    > packrat::restore()

之後重新啟動 R session即可開始作業，詳細使用方法可以至套件官方網站查看。

關於程式執行所留下的日誌檔，爬蟲的日誌檔請保留在 data/資料夾底下，格式為Crawler<option>_<日期>.log，option為爬蟲屬性(抓HTML還是pdf)，
如果被爬取網站有 POST 資訊可以紀錄，可新增 crawlered 檔來紀錄發送過哪些 POST，以免下次重複爬取造成主機負擔；至於資料清理的部分，沒有一定的規範，
但是清理的錯誤log檔必須在datacleaning/ 資料夾底下，日後爬蟲系統會搜尋日誌檔以及抓取的資料，所以盡可能按照此規範做開發，python 要製作日誌檔可以
使用logging模組，其他爬蟲專案的plugin.py為日誌檔格式，可依照此檔案做撰寫，謝謝！


[相關說明]

python modules => https://docs.python.org/3/tutorial/modules.html
python class => https://docs.python.org/3/tutorial/classes.html
scrapy官方網站說明 => https://scrapy.org/doc/
python modules: logging => https://docs.python.org/3/howto/logging.html#logging-basic-tutorial
R Library: packrat => https://rstudio.github.io/packrat/